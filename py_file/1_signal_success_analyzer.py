#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
===============================================================================
å‰ç½®è„šæœ¬ï¼šä¿¡å·æ ·æœ¬ç”Ÿæˆï¼ˆæœ€ç»ˆç¨³å®šç‰ˆï¼‰
æ–‡ä»¶åï¼š1_signal_success_analyzer.py
===============================================================================

ã€å”¯ä¸€è¿‡æ»¤è§„åˆ™ï¼ˆå·²é”æ­»ï¼‰ã€‘
- æ—¥çº¿æ•°é‡ < MIN_BARSï¼ˆé»˜è®¤ 80ï¼‰ â†’ ä¸¢å¼ƒ

ã€è„šæœ¬èŒè´£ã€‘
1. éå† data/day ä¸‹çš„æ‰€æœ‰ CSVï¼ˆå«å­ç›®å½•ï¼‰
2. è¯»å–ä½ ä¸‹è½½çš„è‚¡ç¥¨æ—¥çº¿æ•°æ®
3. è°ƒç”¨ indicators.py â†’ calculate_all_signals()
4. è¯†åˆ«çœŸå®å€™é€‰ä¹°å…¥ä¿¡å·
5. same_day ä¹°å…¥ï¼Œè®¡ç®—æœªæ¥ 20 å¤© â‰¥5% æ ‡ç­¾
6. è¾“å‡º Walk-forward å¯ç›´æ¥ä½¿ç”¨çš„æ ·æœ¬ CSV

ã€è¾“å‡ºæ–‡ä»¶ã€‘
output/signal_samples_same_day.csv
===============================================================================
"""

import os
import time
import random
from multiprocessing import Pool, cpu_count
from typing import List, Optional, Dict

import numpy as np
import pandas as pd
from tqdm import tqdm

from indicators import calculate_all_signals


# =============================================================================
# ä¸€ã€è·¯å¾„ä¸æ ¸å¿ƒå‚æ•°ï¼ˆç ”ç©¶åŒºï¼‰
# =============================================================================

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(PROJECT_ROOT, "data", "day")
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "output")
os.makedirs(OUTPUT_DIR, exist_ok=True)

OUTPUT_FILE = os.path.join(OUTPUT_DIR, "signal_samples_same_day.csv")

# ========= æ ¸å¿ƒç ”ç©¶å‚æ•° =========
MIN_BARS = 80              # ğŸ”´ å°‘äº 80 æ ¹æ—¥çº¿ â†’ ç›´æ¥ä¸¢å¼ƒ
FUTURE_DAYS = 20
GAIN_THRESHOLD = 5.0
SIGNAL_COOLDOWN = 5        # åŒä¸€ä¿¡å·å†·å´æœŸï¼ˆäº¤æ˜“æ—¥ï¼‰

# ========= å¹¶è¡Œå‚æ•° =========
NUM_WORKERS = 4
RETRY_TIMES = 3
CHUNK_SIZE = 20


# =============================================================================
# äºŒã€ç»Ÿè®¡è®¡æ•°å™¨ï¼ˆè®©ä½ çœ‹æ¸…å‘ç”Ÿäº†ä»€ä¹ˆï¼‰
# =============================================================================

STATS_TEMPLATE = {
    "total_csv": 0,        # éå†åˆ°çš„ CSV æ€»æ•°
    "read_ok": 0,          # æˆåŠŸè¯»å…¥å¹¶é€šè¿‡ MIN_BARS çš„
    "signal_hit": 0,       # è‡³å°‘å‡ºç°è¿‡ 1 æ¬¡å€™é€‰ä¿¡å·çš„è‚¡ç¥¨
    "sample_rows": 0,      # æœ€ç»ˆè¾“å‡ºçš„æ ·æœ¬è¡Œæ•°
}


# =============================================================================
# ä¸‰ã€å·¥å…·å‡½æ•°
# =============================================================================

def apply_cooldown(indices: List[int], cooldown: int) -> List[int]:
    """ä¿¡å·å†·å´æœŸï¼Œé˜²æ­¢è¿ç»­å¤šå¤©é‡å¤è®¡æ•°ã€‚"""
    out = []
    last = -10**9
    for i in indices:
        if i - last >= cooldown:
            out.append(i)
            last = i
    return out


def compute_future_gain_same_day(df: pd.DataFrame) -> pd.Series:
    """
    same_day ä¹°å…¥çš„æœªæ¥ 20 å¤©æœ€å¤§æ¶¨å¹…ï¼ˆæ— æœªæ¥å‡½æ•°ï¼‰

    entry = close[t]
    future_high = max(high[t+1 ... t+20])
    """
    entry = df["close"]
    future_high = (
        df["high"]
        .shift(-1)
        .rolling(window=FUTURE_DAYS, min_periods=FUTURE_DAYS)
        .max()
        .shift(-(FUTURE_DAYS - 1))
    )
    return (future_high - entry) / entry * 100


# =============================================================================
# å››ã€è¯»å–å¹¶æ ‡å‡†åŒ–å•åªè‚¡ç¥¨æ•°æ®
# =============================================================================

CN_COL_MAP = {
    "åç§°": "name",
    "æ—¥æœŸ": "date",
    "å¼€ç›˜": "open",
    "æ”¶ç›˜": "close",
    "æœ€é«˜": "high",
    "æœ€ä½": "low",
    "æˆäº¤é‡": "volume",
    "æˆäº¤é¢": "amount",
    "æŒ¯å¹…": "amplitude",
    "æ¶¨è·Œå¹…": "pct_chg",
    "æ¶¨è·Œé¢": "chg",
    "æ¢æ‰‹ç‡": "turnover",
}


def read_stock_csv(path: str) -> Optional[pd.DataFrame]:
    """
    åªåšä¸‰ä»¶äº‹ï¼š
    1. è¯» CSV
    2. åˆ—åæ ‡å‡†åŒ–
    3. æ£€æŸ¥ MIN_BARS
    """
    try:
        df = pd.read_csv(path)
    except Exception:
        return None

    df = df.rename(columns={k: v for k, v in CN_COL_MAP.items() if k in df.columns})

    if "date" not in df.columns:
        return None

    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df = df.dropna(subset=["date"]).sort_values("date").reset_index(drop=True)

    # å¿…è¦å­—æ®µï¼ˆåªè¦ OHLCVï¼‰
    need_cols = {"open", "high", "low", "close", "volume"}
    if not need_cols.issubset(df.columns):
        return None

    # å¼ºåˆ¶æ•°å€¼åŒ–
    for c in need_cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")

    df = df.replace([np.inf, -np.inf], np.nan).dropna()

    # ğŸ”´ å”¯ä¸€è¿‡æ»¤è§„åˆ™
    if len(df) < MIN_BARS:
        return None

    if "name" not in df.columns:
        df["name"] = ""

    return df


# =============================================================================
# äº”ã€æ ¸å¿ƒï¼šå•åªè‚¡ç¥¨å¤„ç†ï¼ˆWorkerï¼‰
# =============================================================================

def process_single_stock(csv_path: str) -> Optional[pd.DataFrame]:
    for attempt in range(1, RETRY_TIMES + 1):
        try:
            df = read_stock_csv(csv_path)
            if df is None:
                return None

            stock_code = os.path.splitext(os.path.basename(csv_path))[0]
            stock_name = str(df["name"].iloc[-1])

            # === è®¡ç®—çœŸå®æŒ‡æ ‡ä¸ä¿¡å· ===
            df = calculate_all_signals(df)

            # === å€™é€‰ä¿¡å·æ± ï¼ˆä½ å¯éšæ—¶åŠ å‡ï¼‰===
            SIGNAL_MAP = {
                "six_veins": df.index[df.get("six_veins_buy", False)].tolist(),
                "chan_buy1": df.index[df.get("chan_buy1", False)].tolist(),
                "chan_buy2": df.index[df.get("chan_buy2", False)].tolist(),
                "chan_buy3": df.index[df.get("chan_buy3", False)].tolist(),
                "combo_steady": df.index[df.get("combo_steady", False)].tolist(),
                "combo_resonance": df.index[df.get("combo_resonance", False)].tolist(),
            }

            df["future_gain_20d"] = compute_future_gain_same_day(df)
            df["success_20d"] = (df["future_gain_20d"] >= GAIN_THRESHOLD).astype(int)

            records = []

            FEATURE_COLS = [
                "close",
                "volume",
                "ma20_ma60_ratio",
                "close_ma20_ratio",
                "close_hhv20_ratio",
                "rsi14",
                "macd_diff",
                "macd_hist",
                "vol_ma5_ratio",
                "vol_ma20_ratio",
                "atr14_ratio",
            ]

            for signal_name, idx_list in SIGNAL_MAP.items():
                idx_list = apply_cooldown(idx_list, SIGNAL_COOLDOWN)

                for idx in idx_list:
                    if pd.isna(df.at[idx, "future_gain_20d"]):
                        continue

                    # ç‰¹å¾å®Œæ•´æ€§æ ¡éªŒ
                    if any(c not in df.columns or pd.isna(df.at[idx, c]) for c in FEATURE_COLS):
                        continue

                    rec = {
                        "stock": stock_code,
                        "name": stock_name,
                        "signal_date": df.at[idx, "date"],
                        "signal_type": signal_name,
                        "entry_price": float(df.at[idx, "close"]),
                        "future_gain_20d": float(df.at[idx, "future_gain_20d"]),
                        "success_20d": int(df.at[idx, "success_20d"]),
                    }

                    for c in FEATURE_COLS:
                        rec[c] = float(df.at[idx, c])

                    records.append(rec)

            return pd.DataFrame.from_records(records) if records else None

        except Exception:
            if attempt == RETRY_TIMES:
                return None
            time.sleep(0.2 * attempt + random.random() * 0.2)


# =============================================================================
# å…­ã€ä¸»ç¨‹åº
# =============================================================================

def main():
    stats = STATS_TEMPLATE.copy()

    # éå†æ‰€æœ‰ CSVï¼ˆå«å­ç›®å½•ï¼‰
    csv_files = []
    for root, _, files in os.walk(DATA_DIR):
        for f in files:
            if f.lower().endswith(".csv"):
                csv_files.append(os.path.join(root, f))

    stats["total_csv"] = len(csv_files)

    print(f"[INFO] å‘ç° CSV æ€»æ•°: {stats['total_csv']}")
    print(f"[INFO] MIN_BARS = {MIN_BARS}")
    print(f"[INFO] å¹¶è¡Œè¿›ç¨‹æ•°: {NUM_WORKERS}")

    all_samples = []

    with Pool(NUM_WORKERS) as pool:
        for res in tqdm(
            pool.imap_unordered(process_single_stock, csv_files, chunksize=CHUNK_SIZE),
            total=len(csv_files),
            desc="ç”Ÿæˆä¿¡å·æ ·æœ¬"
        ):
            if res is not None and not res.empty:
                stats["signal_hit"] += 1
                stats["sample_rows"] += len(res)
                all_samples.append(res)

    if not all_samples:
        print("[ERROR] æ²¡æœ‰ç”Ÿæˆä»»ä½•ä¿¡å·æ ·æœ¬")
        return

    df_all = pd.concat(all_samples, ignore_index=True)
    df_all["signal_date"] = pd.to_datetime(df_all["signal_date"]).dt.strftime("%Y-%m-%d")
    df_all.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")

    print("\n========== è¿è¡Œç»Ÿè®¡ ==========")
    print(f"CSV æ€»æ•°            : {stats['total_csv']}")
    print(f"è‡³å°‘å‘½ä¸­ 1 æ¬¡ä¿¡å·çš„è‚¡ç¥¨æ•° : {stats['signal_hit']}")
    print(f"æœ€ç»ˆæ ·æœ¬è¡Œæ•°         : {stats['sample_rows']}")
    print(f"è¾“å‡ºæ–‡ä»¶            : {OUTPUT_FILE}")
    print("================================")


if __name__ == "__main__":
    main()